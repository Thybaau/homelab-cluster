# k3s Ansible Deployment

DÃ©ploiement automatisÃ© d'un cluster k3s sur des VMs Ubuntu 24.04 hÃ©bergÃ©es sur Proxmox, avec orchestration Ansible et workflows GitHub Actions.

## ğŸ“‹ Table des MatiÃ¨res

- [Vue d'Ensemble](#vue-densemble)
- [PrÃ©requis](#prÃ©requis)
- [Structure du Projet](#structure-du-projet)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [Infrastructure avec Helmfile](#infrastructure-avec-helmfile)
- [Workflows GitHub Actions](#workflows-github-actions)
- [SÃ©curitÃ©](#sÃ©curitÃ©)
- [DÃ©pannage](#dÃ©pannage)

## ğŸ¯ Vue d'Ensemble

Ce projet automatise le dÃ©ploiement d'un cluster k3s avec:
- **1 nÅ“ud master** (k3s-master)
- **N nÅ“uds workers** (k3s-worker-01+)
- **Outils GitOps** (kubectl, helm, helmfile) installÃ©s sur le master
- **Infrastructure automatisÃ©e** (ArgoCD) via Helmfile
- **Workflows CI/CD** pour dÃ©ploiement et audit de sÃ©curitÃ©

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Machine de ContrÃ´le Ansible                â”‚
â”‚                  (ou GitHub Runner)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ SSH
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚            â”‚            â”‚               â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
   â”‚ Master  â”‚  â”‚Worker-01â”‚  â”‚Worker-02â”‚   â”‚Worker-0Nâ”‚
   â”‚ :6443   â”‚  â”‚         â”‚  â”‚         â”‚   â”‚         â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚            â”‚            â”‚             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  k3s Cluster Network
```

## ğŸ“ Structure du Projet
```
.
â”œâ”€â”€ ansible/
â”‚   â”œâ”€â”€ inventory.ini              # Inventaire des hÃ´tes
â”‚   â”œâ”€â”€ playbook.yml               # Playbook principal
â”‚   â”œâ”€â”€ ansible.cfg                # Configuration Ansible
â”‚   â”œâ”€â”€ group_vars/
â”‚   â”‚   â”œâ”€â”€ all.yml                # Variables globales
â”‚   â”‚   â””â”€â”€ k3s_cluster.yml        # Variables du cluster
â”‚   â””â”€â”€ roles/
â”‚       â”œâ”€â”€ prepare_nodes/         # PrÃ©paration systÃ¨me
â”‚       â”œâ”€â”€ k3s_master/            # Installation master
â”‚       â”œâ”€â”€ k3s_workers/           # Installation workers
â”‚       â”œâ”€â”€ gitops_tools/          # kubectl & helm
â”‚       â””â”€â”€ verify_cluster/        # VÃ©rification
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ deploy.yml             # Workflow de dÃ©ploiement
â”‚       â””â”€â”€ security-audit.yml     # Audit de sÃ©curitÃ©
â”œâ”€â”€ output/                        # Kubeconfig & token (local)
â”œâ”€â”€ docs/                          # Documentation
â””â”€â”€ README.md
```

## ğŸš€ Installation

### 1. Cloner le Repository

```bash
git clone <repository-url>
cd homelab-cluster
```

### 2. Configurer l'Inventaire

Ã‰diter `ansible/inventory.ini` avec vos IPs:

```ini
[k3s_master]
k3s-master ansible_host=192.168.1.102

[k3s_workers]
k3s-worker-01 ansible_host=192.168.1.103
k3s-worker-02 ansible_host=192.168.1.104
# Ajouter d'autres workers si nÃ©cessaire

[k3s_cluster:children]
k3s_master
k3s_workers

[k3s_cluster:vars]
ansible_user=k3s
```

### 3. VÃ©rifier la ConnectivitÃ©

```bash
cd ansible
ansible all -i inventory.ini -m ping
```

RÃ©sultat attendu:
```
k3s-master | SUCCESS => {"ping": "pong"}
k3s-worker-01 | SUCCESS => {"ping": "pong"}
...
```

## ğŸ’» Utilisation

### DÃ©ploiement Local

#### DÃ©ploiement Complet

```bash
cd ansible
ansible-playbook -i inventory.ini playbook.yml
```

#### DÃ©ploiement par Ã‰tapes

```bash
# 1. PrÃ©parer les nÅ“uds uniquement
ansible-playbook -i inventory.ini playbook.yml --tags prepare

# 2. Installer le master uniquement
ansible-playbook -i inventory.ini playbook.yml --tags master

# 3. Installer les workers uniquement
ansible-playbook -i inventory.ini playbook.yml --tags workers

# 4. Installer les outils GitOps
ansible-playbook -i inventory.ini playbook.yml --tags gitops

# 5. VÃ©rifier le cluster
ansible-playbook -i inventory.ini playbook.yml --tags verify
```

#### Mode Verbose

```bash
# Afficher les dÃ©tails d'exÃ©cution
ansible-playbook -i inventory.ini playbook.yml -v

# Mode debug complet
ansible-playbook -i inventory.ini playbook.yml -vvv
```

### RÃ©cupÃ©ration du Kubeconfig

AprÃ¨s le dÃ©ploiement, rÃ©cupÃ¨rer le kubeconfig depuis le master:

```bash
# RÃ©cupÃ©rer le kubeconfig
ssh k3s@192.168.1.102 "sudo cat /etc/rancher/k3s/k3s.yaml" | \
  sed 's/127.0.0.1/192.168.1.102/g' > ~/.kube/k3s-config

# SÃ©curiser les permissions
chmod 600 ~/.kube/k3s-config

# Utiliser le cluster
export KUBECONFIG=~/.kube/k3s-config
kubectl get nodes
```

#### Option 2: Configuration Permanente

```bash
# Ajouter Ã  votre ~/.bashrc
echo 'export KUBECONFIG=~/.kube/k3s-config' >> ~/.bashrc
source ~/.bashrc

# Maintenant kubectl utilise automatiquement ce kubeconfig
kubectl get nodes
```

### RÃ©cupÃ©ration du Token (Optionnel)

Si on veut ajouter des workers manuellement:

```bash
# Ou manuellement
ssh k3s@192.168.1.102 "sudo cat /var/lib/rancher/k3s/server/node-token"
```

### VÃ©rification du Cluster

```bash
# VÃ©rifier les nÅ“uds
kubectl get nodes -o wide

# VÃ©rifier les pods systÃ¨me
kubectl get pods -A

# VÃ©rifier la version
kubectl version

# Tester helm
helm version
```

## ğŸš¢ Infrastructure avec Helmfile

Le systÃ¨me dÃ©ploie automatiquement les composants d'infrastructure du cluster en utilisant Helmfile.

### Composants Disponibles

- **ArgoCD** (activÃ© par dÃ©faut) : DÃ©ploiement continu GitOps
- **Sealed Secrets** (dÃ©sactivÃ©) : Gestion des secrets chiffrÃ©s
- **Cert-Manager** (dÃ©sactivÃ©) : Gestion des certificats TLS
- **Prometheus** (dÃ©sactivÃ©) : Surveillance et alertes

### Configuration

Les composants sont dÃ©finis dans `helmfile.yaml` Ã  la racine du projet. Pour activer/dÃ©sactiver un composant, modifier le champ `installed`:

```yaml
releases:
  - name: argocd
    namespace: argocd
    chart: argo/argo-cd
    version: 5.51.6
    installed: true  # true = activÃ©, false = dÃ©sactivÃ©
```

### AccÃ¨s Ã  ArgoCD

AprÃ¨s le dÃ©ploiement, ArgoCD est accessible via NodePort :

```bash
# Obtenir le port NodePort
kubectl get service argocd-server -n argocd

# Obtenir le mot de passe admin
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d

# AccÃ©der Ã  l'interface web
https://<master-ip>:<nodeport>
```

Identifiants par dÃ©faut :
- Utilisateur : `admin`
- Mot de passe : Voir commande ci-dessus

### Ajouter un Composant

1. Modifier `helmfile.yaml` Ã  la racine du projet :
```yaml
releases:
  - name: sealed-secrets
    namespace: sealed-secrets
    chart: sealed-secrets/sealed-secrets
    version: 2.13.2
    installed: true  # Activer le composant
```

2. ExÃ©cuter le playbook :
```bash
cd ansible
ansible-playbook -i inventory.ini playbook.yml --tags gitops
```

3. VÃ©rifier le dÃ©ploiement :
```bash
kubectl get pods -n sealed-secrets
```

### Mise Ã  Jour des Composants

Pour mettre Ã  jour la version d'un composant :

1. Modifier la version dans `helmfile.yaml`
2. ExÃ©cuter : `ansible-playbook -i inventory.ini playbook.yml --tags gitops`
3. Helmfile dÃ©tecte le changement et met Ã  jour le composant

### Gestion Manuelle

Sur le nÅ“ud master, on peut gÃ©rer l'infrastructure manuellement :

```bash
# SSH vers le master
ssh k3s@192.168.1.102

# Afficher l'Ã©tat des releases
helmfile status

# Afficher les diffÃ©rences
helmfile diff

# Synchroniser manuellement
helmfile sync

# Lister les releases
helmfile list

# Supprimer tous les composants
helmfile destroy
```

### DÃ©pannage Helmfile

**Les pods ne dÃ©marrent pas** :
```bash
kubectl get events -n <namespace> --sort-by='.lastTimestamp'
kubectl logs -n <namespace> <pod-name>
```

**Helmfile Ã©choue** :
```bash
# VÃ©rifier la syntaxe du manifest
helmfile lint

# Afficher les logs dÃ©taillÃ©s
helmfile sync --debug
```

**RÃ©initialiser l'infrastructure** :
```bash
# Supprimer les namespaces
kubectl delete namespace argocd sealed-secrets cert-manager monitoring

# Re-dÃ©ployer
cd ansible
ansible-playbook -i inventory.ini playbook.yml --tags gitops
```

## ğŸ¤– Workflows GitHub Actions

### Workflow de DÃ©ploiement

**Fichier**: `.github/workflows/deploy.yml`

**DÃ©clenchement**:
- Manuel via l'interface GitHub (Actions â†’ Deploy k3s Cluster â†’ Run workflow)
- Automatique sur push vers `main` (si fichiers Ansible modifiÃ©s)

**AprÃ¨s le dÃ©ploiement**:

Le workflow affiche les instructions pour rÃ©cupÃ©rer le kubeconfig. Depuis la machine locale:

```bash
ssh k3s@192.168.1.102 "sudo cat /etc/rancher/k3s/k3s.yaml" | \
  sed 's/127.0.0.1/192.168.1.102/g' > ~/.kube/k3s-config
chmod 600 ~/.kube/k3s-config

# Utiliser le cluster
export KUBECONFIG=~/.kube/k3s-config
kubectl get nodes
```

### Workflow d'Audit de SÃ©curitÃ©

**Fichier**: `.github/workflows/security-audit.yml`

**DÃ©clenchement**:
- Automatique sur push/PR (si fichiers Ansible modifiÃ©s)
- Manuel via l'interface GitHub
- PlanifiÃ© quotidiennement Ã  2h du matin

**Ce qui est vÃ©rifiÃ©**:
- âœ… Mots de passe en dur
- âœ… Tokens et clÃ©s API
- âœ… ClÃ©s SSH privÃ©es
- âœ… Credentials AWS
- âœ… Mots de passe SSH/sudo
- âœ… Utilisation d'Ansible Vault
- âœ… Utilisation de variables

## ğŸ” DÃ©pannage

### ProblÃ¨me: Installation k3s Ã‰choue

```bash
# VÃ©rifier les logs sur le nÅ“ud
ssh k3s@192.168.1.102
sudo journalctl -u k3s -f

# VÃ©rifier l'Ã©tat du service
sudo systemctl status k3s
```

### ProblÃ¨me: Workers Ne Rejoignent Pas le Cluster

```bash
# VÃ©rifier la connectivitÃ© au master
ssh k3s@192.168.1.103
curl -k https://192.168.1.102:6443

# VÃ©rifier le token
ssh k3s@192.168.1.102
sudo cat /var/lib/rancher/k3s/server/node-token

# VÃ©rifier les logs du worker
ssh k3s@192.168.1.103
sudo journalctl -u k3s-agent -f
```

### ProblÃ¨me: NÅ“uds en Ã‰tat NotReady

```bash
# VÃ©rifier les pods systÃ¨me
kubectl get pods -n kube-system

# VÃ©rifier les Ã©vÃ©nements
kubectl get events -A --sort-by='.lastTimestamp'

# DÃ©crire un nÅ“ud
kubectl describe node k3s-worker-01
```

### RÃ©initialiser le Cluster

```bash
# Sur chaque nÅ“ud (master et workers)
ssh k3s@<node-ip>
sudo /usr/local/bin/k3s-uninstall.sh  # sur le master
sudo /usr/local/bin/k3s-agent-uninstall.sh  # sur les workers

# Relancer le dÃ©ploiement
ansible-playbook -i inventory.ini playbook.yml
```

## ğŸ“ Licence

Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.
